import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import time
import os
from pathlib import Path

# EXAONE ëª¨ë¸ ê²½ë¡œ (config/settings.pyì—ì„œ ì§€ì •í–ˆë˜ ê²ƒê³¼ ë™ì¼í•˜ê²Œ)
EXAONE_LOCAL_PATH = Path("./models/exaone_deep")  # í•„ìš”ì‹œ ì ˆëŒ€ê²½ë¡œë¡œ ìˆ˜ì •
TEST_PROMPT = """ë‹¹ì‹ ì€ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LGì „ìì— ëŒ€í•œ íˆ¬ì ì˜ê²¬ì„ ì œì‹œí•˜ì„¸ìš”.

ã€ë¶„ì„ ì •ë³´ã€‘
â€¢ ê¸ì • ë‰´ìŠ¤: 5ê°œ
â€¢ ë¶€ì • ë‰´ìŠ¤: 2ê°œ
â€¢ ì¤‘ë¦½ ë‰´ìŠ¤: 3ê°œ
â€¢ ì´ ë‰´ìŠ¤: 10ê°œ
â€¢ í˜„ì¬ê°€: 110,000ì›
â€¢ ë³€ë™ë¥ : +1.5%
â€¢ ìƒíƒœ: ìƒìŠ¹
â€¢ ì°¨íŠ¸ íŠ¸ë Œë“œ: ê°•í•œ ìƒìŠ¹

ã€ìš”ì²­ì‚¬í•­ã€‘
ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.

íˆ¬ìì¶”ì²œ: ë§¤ìˆ˜/ë³´ë¥˜/ë§¤ë„ ì¤‘ í•˜ë‚˜
í™•ì‹ ë„: 1~10
ë¶„ì„ê·¼ê±°: ì´ìœ ë¥¼ 2ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…

ë‹µë³€:"""

# ë¡œë”© ì‹œì‘
print("ğŸ” EXAONE ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
if not EXAONE_LOCAL_PATH.exists():
    raise FileNotFoundError(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {EXAONE_LOCAL_PATH}")

# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
start = time.time()
tokenizer = AutoTokenizer.from_pretrained(str(EXAONE_LOCAL_PATH), trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    str(EXAONE_LOCAL_PATH),
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    low_cpu_mem_usage=True,
    trust_remote_code=True,
)
load_time = time.time() - start
print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({load_time:.2f}ì´ˆ)")

# í…ìŠ¤íŠ¸ ìƒì„±
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

inputs = tokenizer(TEST_PROMPT, return_tensors="pt", max_length=1024, truncation=True)
inputs = {k: v.to(model.device) for k, v in inputs.items()}

print("ğŸš€ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘...")
start = time.time()
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.3,
        do_sample=True,
        top_p=0.85,
        repetition_penalty=1.2,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id
    )
gen_time = time.time() - start
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(f"âœ… í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ ({gen_time:.2f}ì´ˆ)")
print("\nğŸ“„ EXAONE ì‘ë‹µ:\n" + "-" * 40)
print(generated_text)
print("-" * 40)
