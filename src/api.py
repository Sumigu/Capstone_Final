from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import os
import glob
from typing import List, Dict, Any, Optional
import sys
import time
import requests
from bs4 import BeautifulSoup
import sqlite3
from datetime import datetime, timedelta, date as date_module, timezone
import traceback
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
from config.settings import *
from src.sentiment_analyzer import SentimentAnalyzer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=getattr(logging, LOG_LEVEL), format=LOG_FORMAT)
logger = logging.getLogger(__name__)

app = FastAPI(title="ì£¼ì‹ ë‰´ìŠ¤ AI ë¶„ì„ API (KR-FinBERT + EXAONE + TossPay)")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ê°ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
sentiment_analyzer = None

def init_database():
    try:
        os.makedirs(os.path.dirname(DATABASE_PATH), exist_ok=True)
        
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        cursor.execute('DROP TABLE IF EXISTS daily_sentiment_summary')
        cursor.execute('DROP TABLE IF EXISTS news_data')
        cursor.execute('DROP TABLE IF EXISTS trading_recommendations')
        
        cursor.execute('''
            CREATE TABLE news_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                company_code TEXT,
                company_name TEXT,
                title TEXT,
                url TEXT,
                source TEXT,
                published_date DATE,
                crawled_date DATE,
                sentiment TEXT,
                sentiment_prob REAL,
                model_used TEXT,
                analysis_reason TEXT,
                investment_impact TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE daily_sentiment_summary (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                company_code TEXT,
                company_name TEXT,
                analysis_date DATE,
                total_news INTEGER,
                positive_count INTEGER,
                negative_count INTEGER,
                neutral_count INTEGER,
                positive_ratio REAL,
                negative_ratio REAL,
                neutral_ratio REAL,
                sentiment_score REAL,
                prev_day_sentiment_score REAL,
                sentiment_change REAL,
                sentiment_trend TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE trading_recommendations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                company_code TEXT,
                company_name TEXT,
                recommendation TEXT,
                confidence REAL,
                reason TEXT,
                strategy TEXT,
                risk_analysis TEXT,
                period TEXT,
                ai_generated INTEGER,
                model_used TEXT,
                full_response TEXT,
                sentiment_summary TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('CREATE INDEX idx_news_company_date ON news_data(company_code, published_date)')
        cursor.execute('CREATE INDEX idx_summary_company_date ON daily_sentiment_summary(company_code, analysis_date)')
        
        conn.commit()
        conn.close()
        logger.info("âœ… Database schema updated successfully")
        
    except Exception as e:
        logger.error(f"âŒ Database initialization error: {e}")
        raise e

def parse_news_date(date_str):
    try:
        if '.' in date_str and len(date_str.split('.')) == 3:
            return datetime.strptime(date_str, '%Y.%m.%d').date()
        elif '.' in date_str and len(date_str.split('.')) == 2:
            current_year = date_module.today().year
            return datetime.strptime(f"{current_year}.{date_str}", '%Y.%m.%d').date()
        else:
            return date_module.today()
    except:
        return date_module.today()

def save_news_to_db_with_date(company_code: str, company_name: str, news_df: pd.DataFrame, crawled_date):
    """í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        saved_count = 0
        for _, row in news_df.iterrows():
            try:
                cursor.execute('''
                    SELECT id FROM news_data 
                    WHERE company_code = ? AND title = ? AND crawled_date = ?
                ''', (company_code, row['ì œëª©'], crawled_date))
                
                if cursor.fetchone() is None:
                    published_date = parse_news_date(row['ë‚ ì§œ'])
                    
                    cursor.execute('''
                        INSERT INTO news_data 
                        (company_code, company_name, title, url, source, published_date, crawled_date,
                         sentiment, sentiment_prob, model_used, analysis_reason, investment_impact)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        company_code, company_name, row['ì œëª©'], row.get('URL', ''), 
                        row['ì–¸ë¡ ì‚¬'], published_date, crawled_date, row['sentiment'], 
                        row['sentiment_prob'], row.get('model_used', 'unknown'),
                        row.get('analysis_reason', ''), row.get('investment_impact', 'ë¶„ì„ ëŒ€ê¸°')
                    ))
                    saved_count += 1
            except Exception as row_error:
                logger.error(f"Error saving TossPay news row to DB: {row_error}")
                continue
        
        conn.commit()
        conn.close()
        logger.info(f"Saved {saved_count} new TossPay news items to database")
        
    except Exception as e:
        logger.error(f"TossPay news database save error: {e}")

def calculate_and_save_daily_sentiment(company_code: str, company_name: str, analysis_date):
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT sentiment, COUNT(*) as count
            FROM news_data 
            WHERE company_code = ? AND crawled_date = ?
            GROUP BY sentiment
        ''', (company_code, analysis_date))
        
        sentiment_counts = dict(cursor.fetchall())
        
        positive_count = sentiment_counts.get('ê¸ì •', 0)
        negative_count = sentiment_counts.get('ë¶€ì •', 0)
        neutral_count = sentiment_counts.get('ì¤‘ë¦½', 0)
        total_news = positive_count + negative_count + neutral_count
        
        if total_news == 0:
            return
        
        positive_ratio = positive_count / total_news
        negative_ratio = negative_count / total_news
        neutral_ratio = neutral_count / total_news
        sentiment_score = (positive_count - negative_count) / total_news
        
        prev_date = analysis_date - timedelta(days=1)
        
        cursor.execute('''
            SELECT sentiment_score FROM daily_sentiment_summary 
            WHERE company_code = ? AND analysis_date = ?
        ''', (company_code, prev_date))
        
        prev_result = cursor.fetchone()
        prev_day_sentiment_score = prev_result[0] if prev_result else None
        
        sentiment_change = None
        sentiment_trend = 'stable'
        
        if prev_day_sentiment_score is not None:
            sentiment_change = sentiment_score - prev_day_sentiment_score
            if sentiment_change > 0.1:
                sentiment_trend = 'improving'
            elif sentiment_change < -0.1:
                sentiment_trend = 'declining'
            else:
                sentiment_trend = 'stable'
        
        cursor.execute('''
            DELETE FROM daily_sentiment_summary 
            WHERE company_code = ? AND analysis_date = ?
        ''', (company_code, analysis_date))
        
        cursor.execute('''
            INSERT INTO daily_sentiment_summary 
            (company_code, company_name, analysis_date, total_news, 
             positive_count, negative_count, neutral_count,
             positive_ratio, negative_ratio, neutral_ratio,
             sentiment_score, prev_day_sentiment_score, sentiment_change, sentiment_trend)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            company_code, company_name, analysis_date, total_news,
            positive_count, negative_count, neutral_count,
            positive_ratio, negative_ratio, neutral_ratio,
            sentiment_score, prev_day_sentiment_score, sentiment_change, sentiment_trend
        ))
        
        conn.commit()
        conn.close()
        
        if sentiment_change is not None:
            logger.info(f"âœ… Daily sentiment summary saved: {sentiment_score:.3f} (change: {sentiment_change:+.3f})")
        else:
            logger.info(f"âœ… Daily sentiment summary saved: {sentiment_score:.3f} (change: N/A - ì²« ë‚  ë°ì´í„°)")
        
    except Exception as e:
        logger.error(f"Error calculating daily sentiment: {e}")

def fetch_tosspay_news(company_code: str, size: int = 100, order_by: str = 'relevant'):
    """í† ìŠ¤í˜ì´ ì¦ê¶Œ ë‰´ìŠ¤ APIì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    try:
        url = f"https://wts-info-api.tossinvest.com/api/v2/news/companies/{company_code}?size={size}&orderBy={order_by}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
            "Referer": "https://tossinvest.com/"
        }
        
        logger.info(f"Fetching TossPay news from: {url}")
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        data = response.json()
        logger.info(f"TossPay API response status: {response.status_code}")
        
        news_list = []
        
        # API ì‘ë‹µ êµ¬ì¡°ì— ë”°ë¼ ë°ì´í„° ì¶”ì¶œ
        result = data.get('result', {})
        body = result.get('body', [])
        
        logger.info(f"Found {len(body)} news items from TossPay API")
        
        for item in body:
            try:
                title = item.get('title', '').strip()
                source_info = item.get('source', {})
                source_name = source_info.get('name', '').strip()
                created_at = item.get('createdAt', '')
                news_id = item.get('id', '')
                
                # ISO 8601 í˜•ì‹ì˜ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
                created_at_dt = None
                if created_at:
                    try:
                        # 'Z'ê°€ ìˆìœ¼ë©´ UTCë¡œ ì²˜ë¦¬, ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì²˜ë¦¬
                        if created_at.endswith('Z'):
                            created_at_dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                        else:
                            created_at_dt = datetime.fromisoformat(created_at)
                        
                        # í•œêµ­ ì‹œê°„ìœ¼ë¡œ ë³€í™˜ (UTC+9)
                        if created_at_dt.tzinfo is not None:
                            kst = timezone(timedelta(hours=9))
                            created_at_dt = created_at_dt.astimezone(kst)
                        
                        # ë‚ ì§œ í˜•ì‹ì„ ê¸°ì¡´ ë„¤ì´ë²„ í˜•ì‹ê³¼ ë§ì¶”ê¸° (YYYY.MM.DD)
                        formatted_date = created_at_dt.strftime('%Y.%m.%d')
                        
                    except Exception as date_error:
                        logger.warning(f"Date parsing error for {created_at}: {date_error}")
                        formatted_date = datetime.now().strftime('%Y.%m.%d')
                else:
                    formatted_date = datetime.now().strftime('%Y.%m.%d')
                
                if title and source_name:  # ì œëª©ê³¼ ì–¸ë¡ ì‚¬ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                    news_list.append({
                        'ì œëª©': title,
                        'ì–¸ë¡ ì‚¬': source_name,
                        'ë‚ ì§œ': formatted_date,
                        'URL': f"https://tossinvest.com/news/{news_id}" if news_id else ""
                    })
                    
            except Exception as item_error:
                logger.warning(f"Error processing TossPay news item: {item_error}")
                continue
        
        logger.info(f"Successfully processed {len(news_list)} TossPay news items")
        return news_list
        
    except requests.RequestException as e:
        logger.error(f"Network error fetching TossPay news: {e}")
        return []
    except Exception as e:
        logger.error(f"Error fetching TossPay news: {e}")
        return []

@app.on_event("startup")
async def startup_event():
    global sentiment_analyzer
    
    init_database()
    
    logger.info("ğŸ¯ Initializing AI Pipeline: KR-FinBERT (1ë‹¨ê³„) + EXAONE (2ë‹¨ê³„)")
    sentiment_analyzer = SentimentAnalyzer()
    logger.info("âœ… AI Pipeline initialized: ê°ì„±ë¶„ì„ + íˆ¬ì ì¸ì‚¬ì´íŠ¸")

@app.get("/")
def read_root():
    return {
        "message": "ì£¼ì‹ ë‰´ìŠ¤ AI ë¶„ì„ API (KR-FinBERT + EXAONE + TossPay)", 
        "ai_pipeline": {
            "stage_1": "KR-FinBERT: í•œêµ­ì–´ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ë¥˜",
            "stage_2": "EXAONE: ê°ì„± ê²°ê³¼ + ì°¨íŠ¸ â†’ íˆ¬ì ì¸ì‚¬ì´íŠ¸"
        },
        "features": [
            "1ë‹¨ê³„: KR-FinBERT í•œêµ­ì–´ ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” ê°ì„± ë¶„ì„",
            "2ë‹¨ê³„: EXAONE: ê°ì„± ê²°ê³¼ + ì°¨íŠ¸ â†’ íˆ¬ì ì¸ì‚¬ì´íŠ¸",
            "í† ìŠ¤í˜ì´ ì¦ê¶Œ ë‰´ìŠ¤ API ì—°ë™",
            "ë„¤ì´ë²„ ê¸ˆìœµ ì‹¤ì‹œê°„ ì£¼ê°€ ì°¨íŠ¸",
            "ë‰´ìŠ¤ í¬ë¡¤ë§ ë° DB ì €ì¥",
            "2ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ íˆ¬ì íŒë‹¨"
        ],
        "data_sources": {
            "news": "TossPay Securities News API",
            "stock_price": "Naver Finance",
            "chart": "Naver Finance"
        },
        "version": "2.1.0-TOSSPAY-NEWS",
        "status": "healthy"
    }

@app.get("/companies")
def get_companies():
    """ì£¼ìš” í•œêµ­ ì£¼ì‹ ëª©ë¡ ë°˜í™˜"""
    try:
        stock_list = [
            {"code": "005930", "name": "ì‚¼ì„±ì „ì"},
            {"code": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
            {"code": "207940", "name": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤"},
            {"code": "373220", "name": "LGì—ë„ˆì§€ì†”ë£¨ì…˜"},
            {"code": "012450", "name": "í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤"},
            {"code": "105560", "name": "KBê¸ˆìœµ"},
            {"code": "005380", "name": "í˜„ëŒ€ì°¨"},
            {"code": "329180", "name": "HDí˜„ëŒ€ì¤‘ê³µì—…"},
            {"code": "005935", "name": "ì‚¼ì„±ì „ììš°"},
            {"code": "000270", "name": "ê¸°ì•„"},
            {"code": "068270", "name": "ì…€íŠ¸ë¦¬ì˜¨"},
            {"code": "035420", "name": "NAVER"},
            {"code": "055550", "name": "ì‹ í•œì§€ì£¼"},
            {"code": "034020", "name": "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°"},
            {"code": "028260", "name": "ì‚¼ì„±ë¬¼ì‚°"},
            {"code": "042660", "name": "í•œí™”ì˜¤ì…˜"},
            {"code": "012330", "name": "í˜„ëŒ€ëª¨ë¹„ìŠ¤"},
            {"code": "011200", "name": "HMM"},
            {"code": "009540", "name": "HDí•œêµ­ì¡°ì„ í•´ì–‘"},
            {"code": "086790", "name": "í•˜ë‚˜ê¸ˆìœµì§€ì£¼"},
            {"code": "138040", "name": "ë©”ë¦¬ì¸ ê¸ˆìœµì§€ì£¼"},
            {"code": "015760", "name": "í•œêµ­ì „ë ¥"},
            {"code": "032830", "name": "ì‚¼ì„±ìƒëª…"},
            {"code": "005490", "name": "POSCOí™€ë”©ìŠ¤"},
            {"code": "196170", "name": "ì•Œí…Œì˜¤ì  "},
            {"code": "259960", "name": "í¬ë˜í”„í†¤"},
            {"code": "000810", "name": "ì‚¼ì„±í™”ì¬"},
            {"code": "035720", "name": "ì¹´ì¹´ì˜¤"},
            {"code": "064350", "name": "í˜„ëŒ€ë¡œí…œ"},
            {"code": "010130", "name": "ê³ ë ¤ì•„ì—°"},
            {"code": "033780", "name": "KT&G"},
            {"code": "010140", "name": "ì‚¼ì„±ì¤‘ê³µì—…"},
            {"code": "267270", "name": "HDí˜„ëŒ€ì¼ë ‰íŠ¸ë¦­"},
            {"code": "316140", "name": "ìš°ë¦¬ê¸ˆìœµì§€ì£¼"},
            {"code": "402340", "name": "SKìŠ¤í€˜ì–´"},
            {"code": "030200", "name": "KT"},
            {"code": "051910", "name": "LGí™”í•™"},
            {"code": "096770", "name": "SKì´ë…¸ë² ì´ì…˜"},
            {"code": "024110", "name": "ê¸°ì—…ì€í–‰"},
            {"code": "352820", "name": "í•˜ì´ë¸Œ"},
            {"code": "066570", "name": "LGì „ì"},
            {"code": "323410", "name": "ì¹´ì¹´ì˜¤ë±…í¬"},
            {"code": "017670", "name": "SKí…”ë ˆì½¤"},
            {"code": "006400", "name": "ì‚¼ì„±SDI"},
            {"code": "003550", "name": "LG"},
            {"code": "018260", "name": "ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤"},
            {"code": "034730", "name": "SK"},
            {"code": "079550", "name": "LIGë„¥ìŠ¤ì›"},
            {"code": "180640", "name": "í•œì§„ì¹¼"},
            {"code": "009150", "name": "ì‚¼ì„±ì „ê¸°"},
        ]
        
        logger.info(f"Returning stock list: {len(stock_list)} companies")
        return stock_list
        
    except Exception as e:
        logger.error(f"Error returning stock list: {e}")
        return [
            {"code": "005930", "name": "ì‚¼ì„±ì „ì"},
            {"code": "000660", "name": "SKí•˜ì´ë‹‰ìŠ¤"},
            {"code": "035420", "name": "NAVER"}
        ]

@app.get("/stock_price/{company_code}")
def get_stock_price(company_code: str):
    try:
        url = f"https://finance.naver.com/item/main.naver?code={company_code}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        current_price_tag = soup.select_one('div.rate_info div.today p.no_today span.blind')
        current_price = int(current_price_tag.text.replace(',', '')) if current_price_tag else None
        
        change_tag = soup.select_one('div.rate_info div.today p.no_exday em span.blind')
        change_value = change_tag.text.replace(',', '') if change_tag else None
        
        change_rate_tag = soup.select_one('div.rate_info div.today p.no_exday em span.blind:nth-of-type(2)')
        change_rate = change_rate_tag.text if change_rate_tag else None
        
        volume_tag = soup.select_one('div.rate_info table tbody tr:nth-child(3) td span.blind')
        volume = volume_tag.text.replace(',', '') if volume_tag else None
        
        market_cap_tag = soup.select_one('#_market_sum')
        market_cap = market_cap_tag.text.strip().replace('\t', '').replace('\n', '') if market_cap_tag else None
        
        is_rising = soup.select_one('div.rate_info div.today p.no_exday em.no_up') is not None
        is_falling = soup.select_one('div.rate_info div.today p.no_exday em.no_down') is not None
        
        status = "ìƒìŠ¹" if is_rising else "í•˜ë½" if is_falling else "ë³´í•©"
        
        return {
            "company_code": company_code,
            "current_price": current_price,
            "change_value": change_value,
            "change_rate": change_rate,
            "volume": volume,
            "market_cap": market_cap,
            "status": status
        }
    except Exception as e:
        logger.error(f"Error getting stock price for {company_code}: {e}")
        raise HTTPException(status_code=500, detail=f"ì£¼ê°€ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/stock_chart/{company_code}")
def get_stock_chart_data(company_code: str, period: str = "1mo", interval: str = "1d"):
    """ë„¤ì´ë²„ ê¸ˆìœµ ê¸°ë°˜ ì£¼ê°€ ì°¨íŠ¸ ë°ì´í„° ì¡°íšŒ"""
    try:
        logger.info(f"Fetching chart data from Naver Finance for {company_code}")
        
        count_map = {
            "1mo": 30,
            "3mo": 90,
            "6mo": 180,
            "1y": 365
        }
        count = count_map.get(period, 30)
        
        chart_url = f"https://fchart.stock.naver.com/sise.nhn?symbol={company_code}&timeframe=day&count={count}&requestType=0"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Referer": f"https://finance.naver.com/item/main.naver?code={company_code}",
            "Accept": "text/xml, application/xml, application/xhtml+xml, text/html;q=0.9, text/plain;q=0.8, image/png,*/*;q=0.5"
        }
        
        response = requests.get(chart_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'xml')
        items = soup.find_all('item')
        
        if not items:
            raise HTTPException(status_code=404, detail=f"ì¢…ëª© ì½”ë“œ {company_code}ì˜ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        chart_data = []
        for item in items:
            try:
                data_attr = item.get('data')
                if data_attr:
                    parts = data_attr.split('|')
                    if len(parts) >= 6:
                        date_str = parts[0]
                        
                        if len(date_str) == 8:
                            formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                        else:
                            formatted_date = date_str
                        
                        def safe_float(value, default=0.0):
                            try:
                                return float(value) if value and value.strip() else default
                            except:
                                return default
                        
                        def safe_int(value, default=0):
                            try:
                                return int(value) if value and value.strip() else default
                            except:
                                return default
                        
                        chart_data.append({
                            "date": formatted_date,
                            "open": safe_float(parts[1]),
                            "high": safe_float(parts[2]),
                            "low": safe_float(parts[3]),
                            "close": safe_float(parts[4]),
                            "volume": safe_int(parts[5])
                        })
            except Exception as item_error:
                logger.warning(f"Error parsing item: {item_error}")
                continue
        
        if not chart_data:
            raise HTTPException(status_code=404, detail=f"ì¢…ëª© ì½”ë“œ {company_code}ì˜ ìœ íš¨í•œ ì°¨íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        chart_data.sort(key=lambda x: x['date'])
        
        # íŠ¸ë Œë“œ ë¶„ì„
        trend = "ë°ì´í„° ë¶€ì¡±"
        if len(chart_data) >= 2:
            try:
                recent_close = chart_data[-1]['close']
                previous_close = chart_data[-2]['close']
                
                if recent_close > 0 and previous_close > 0:
                    change_ratio = recent_close / previous_close
                    
                    if change_ratio > 1.02:
                        trend = "ê°•í•œ ìƒìŠ¹"
                    elif change_ratio > 1.0:
                        trend = "ìƒìŠ¹"
                    elif change_ratio < 0.98:
                        trend = "ê°•í•œ í•˜ë½"
                    elif change_ratio < 1.0:
                        trend = "í•˜ë½"
                    else:
                        trend = "ë³´í•©"
            except Exception as trend_error:
                logger.warning(f"Error calculating trend: {trend_error}")
                trend = "ê³„ì‚° ì˜¤ë¥˜"
        
        result = {
            "company_code": company_code,
            "data_source": "Naver Finance",
            "period": period,
            "interval": interval,
            "chart_data": chart_data,
            "trend": trend,
            "data_points": len(chart_data)
        }
        
        logger.info(f"Naver chart data successfully processed: {len(chart_data)} points")
        return result
        
    except HTTPException:
        raise
    except requests.RequestException as e:
        logger.error(f"Network error fetching Naver chart data: {e}")
        raise HTTPException(status_code=503, detail="ë„¤ì´ë²„ ê¸ˆìœµ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"Error getting Naver chart data for {company_code}: {e}")
        raise HTTPException(status_code=500, detail=f"ì°¨íŠ¸ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/crawl_news/{company_code}")
def crawl_company_news_optimized(company_code: str, company_name: Optional[str] = None, pages: int = 5):
    """1ë‹¨ê³„: í† ìŠ¤í˜ì´ ë‰´ìŠ¤ APIì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘ í›„ KR-FinBERTë¡œ í•œêµ­ì–´ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„"""
    try:
        today = date_module.today()
        logger.info(f"ğŸ¯ 1ë‹¨ê³„ ì‹œì‘: {company_code} í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° KR-FinBERT ê°ì„± ë¶„ì„")
        
        if not company_name:
            companies = get_companies()
            for company in companies:
                if company["code"] == company_code:
                    company_name = company["name"]
                    break
            if not company_name:
                company_name = company_code
        
        # í† ìŠ¤í˜ì´ ë‰´ìŠ¤ APIì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        news_data = fetch_tosspay_news(company_code, size=min(pages * 20, 100))
        
        if news_data:
            logger.info(f"Total news collected from TossPay: {len(news_data)}")
            news_df = pd.DataFrame(news_data)
            
            news_df['crawled_date'] = today
            
            logger.info("ğŸ¯ 1ë‹¨ê³„: KR-FinBERT í•œêµ­ì–´ ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì‹œì‘...")
            news_df = sentiment_analyzer.analyze_dataframe_optimized(news_df)
            
            sentiment_summary = {
                'positive_count': len(news_df[news_df['sentiment'] == 'ê¸ì •']),
                'negative_count': len(news_df[news_df['sentiment'] == 'ë¶€ì •']),
                'neutral_count': len(news_df[news_df['sentiment'] == 'ì¤‘ë¦½']),
                'top_news': news_df['ì œëª©'].head(5).tolist(),
                'total_news': len(news_df)
            }
            
            save_news_to_db_with_date(company_code, company_name, news_df, today)
            calculate_and_save_daily_sentiment(company_code, company_name, today)
            
            logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: KR-FinBERTê°€ {len(news_df)}ê°œ í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì™„ë£Œ")
            
            return {
                "news_data": news_df.to_dict(orient="records"),
                "sentiment_summary": sentiment_summary,
                "analysis_stage": "1ë‹¨ê³„ ì™„ë£Œ: KR-FinBERT í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ê°ì„± ë¶„ì„",
                "next_stage": "2ë‹¨ê³„ ëŒ€ê¸°: EXAONE ì¢…í•© ì¸ì‚¬ì´íŠ¸",
                "data_source": "TossPay Securities News API"
            }
        else:
            logger.warning("No news data collected from TossPay")
            
            return {
                "news_data": [],
                "sentiment_summary": {
                    'positive_count': 0,
                    'negative_count': 0,
                    'neutral_count': 0,
                    'top_news': [],
                    'total_news': 0
                },
                "analysis_stage": "1ë‹¨ê³„ ì‹¤íŒ¨: í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ",
                "data_source": "TossPay Securities News API"
            }
        
    except Exception as e:
        logger.error(f"Unexpected error in TossPay news analysis: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"1ë‹¨ê³„ í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/comprehensive_insights/{company_code}")
def get_comprehensive_insights(company_code: str, company_name: Optional[str] = None):
    """2ë‹¨ê³„: EXAONEì´ KR-FinBERT ê²°ê³¼ + ì°¨íŠ¸ â†’ ì¢…í•© íˆ¬ì ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    try:
        if not company_name:
            companies = get_companies()
            for company in companies:
                if company["code"] == company_code:
                    company_name = company["name"]
                    break
            if not company_name:
                company_name = company_code
        
        logger.info(f"ğŸ§  2ë‹¨ê³„ ì‹œì‘: EXAONEì´ {company_name} ì¢…í•© íˆ¬ì ì¸ì‚¬ì´íŠ¸ ìƒì„±")
        
        # 1. ë„¤ì´ë²„ ì°¨íŠ¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        chart_data = get_stock_chart_data(company_code, period="1mo", interval="1d")
        chart_trend = chart_data.get('trend', 'ì•Œ ìˆ˜ ì—†ìŒ')
        
        # 2. KR-FinBERT ê°ì„± ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        conn = sqlite3.connect(DATABASE_PATH)
        today = date_module.today()
        
        sentiment_query = '''
            SELECT sentiment, COUNT(*) as count
            FROM news_data 
            WHERE company_code = ? AND crawled_date = ?
            GROUP BY sentiment
        '''
        sentiment_df = pd.read_sql_query(sentiment_query, conn, params=(company_code, today))
        
        news_query = '''
            SELECT title, sentiment, sentiment_prob
            FROM news_data 
            WHERE company_code = ? AND crawled_date = ?
            ORDER BY sentiment_prob DESC, created_at DESC
            LIMIT 10
        '''
        news_df = pd.read_sql_query(news_query, conn, params=(company_code, today))
        conn.close()
        
        # 3. KR-FinBERT ê°ì„± ìš”ì•½ ìƒì„±
        sentiment_summary = {
            'positive_count': 0,
            'negative_count': 0,
            'neutral_count': 0,
            'total_news': 0
        }
        
        for _, row in sentiment_df.iterrows():
            sentiment = row['sentiment']
            count = row['count']
            sentiment_summary['total_news'] += count
            
            if sentiment == 'ê¸ì •':
                sentiment_summary['positive_count'] = count
            elif sentiment == 'ë¶€ì •':
                sentiment_summary['negative_count'] = count
            elif sentiment == 'ì¤‘ë¦½':
                sentiment_summary['neutral_count'] = count
        
        # 4. ë‰´ìŠ¤ ì œëª© ë¦¬ìŠ¤íŠ¸
        news_titles = news_df['title'].tolist() if not news_df.empty else []
        
        # 5. ì£¼ê°€ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        stock_price_data = get_stock_price(company_code)
        
        # 6. EXAONEë¡œ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        logger.info(f"ğŸ§  2ë‹¨ê³„ ì§„í–‰: EXAONEì´ KR-FinBERT ê²°ê³¼({sentiment_summary['total_news']}ê°œ í† ìŠ¤í˜ì´ ë‰´ìŠ¤)ì™€ ì°¨íŠ¸({chart_trend})ë¥¼ ì¢…í•© ë¶„ì„")
        insights = sentiment_analyzer.generate_comprehensive_investment_insight(
            sentiment_summary, stock_price_data, company_name, news_titles, chart_trend
        )
        
        return {
            "company_code": company_code,
            "company_name": company_name,
            "analysis_pipeline": {
                "stage_1": "KR-FinBERT í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ ì™„ë£Œ",
                "stage_2": "EXAONE ê°ì„±+ì°¨íŠ¸ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ"
            },
            "chart_trend": chart_trend,
            "chart_data_source": chart_data.get('data_source', 'Naver Finance'),
                        "kr_finbert_results": {
                "sentiment_summary": sentiment_summary,
                "analysis_method": "í•œêµ­ì–´ ê¸ˆìœµ ë„ë©”ì¸ íŠ¹í™” BERT",
                "data_source": "TossPay Securities News API"
            },
            "news_titles": news_titles[:5],
            "stock_data": {
                "current_price": stock_price_data.get('current_price'),
                "change_rate": stock_price_data.get('change_rate'),
                "status": stock_price_data.get('status')
            },
            "comprehensive_insights": insights,
            "analysis_method": "2ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸: KR-FinBERT(í† ìŠ¤í˜ì´ ë‰´ìŠ¤ ê°ì„±) + EXAONE(ì¢…í•©ì¸ì‚¬ì´íŠ¸)"
        }
        
    except Exception as e:
        logger.error(f"Error in 2-stage AI pipeline: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"2ë‹¨ê³„ AI íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")

@app.get("/sentiment_comparison/{company_code}")
def get_sentiment_comparison(company_code: str):
    try:
        today = date_module.today()
        yesterday = today - timedelta(days=1)
        
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT analysis_date, sentiment_score, positive_ratio, negative_ratio, total_news,
                   sentiment_change, sentiment_trend
            FROM daily_sentiment_summary 
            WHERE company_code = ? AND analysis_date IN (?, ?)
            ORDER BY analysis_date DESC
        ''', (company_code, today, yesterday))
        
        results = cursor.fetchall()
        conn.close()
        
        if len(results) >= 1:
            today_data = results[0]
            yesterday_data = results[1] if len(results) > 1 else None
            
            return {
                "today": {
                    "date": today_data[0],
                    "sentiment_score": today_data[1],
                    "positive_ratio": today_data[2],
                    "negative_ratio": today_data[3],
                    "total_news": today_data[4],
                    "sentiment_change": today_data[5],
                    "sentiment_trend": today_data[6]
                },
                "yesterday": {
                    "date": yesterday_data[0] if yesterday_data else None,
                    "sentiment_score": yesterday_data[1] if yesterday_data else None,
                    "positive_ratio": yesterday_data[2] if yesterday_data else None,
                    "negative_ratio": yesterday_data[3] if yesterday_data else None,
                    "total_news": yesterday_data[4] if yesterday_data else None
                } if yesterday_data else None
            }
        else:
            return {"message": "ì˜¤ëŠ˜ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
            
    except Exception as e:
        logger.error(f"Error getting sentiment comparison: {e}")
        raise HTTPException(status_code=500, detail=f"ê°ì„± ë¹„êµ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "2.1.0-TOSSPAY-NEWS",
        "ai_pipeline": {
            "stage_1": "KR-FinBERT Korean Financial News Sentiment Analysis",
            "stage_2": "EXAONE Comprehensive Investment Insights"
        },
        "data_sources": {
            "news": "TossPay Securities News API",
            "stock_price": "Naver Finance",
            "chart": "Naver Finance"
        },
        "features": ["KR-FinBERT", "EXAONE", "TossPay News", "Naver Charts"],
        "database": str(DATABASE_PATH)
    }